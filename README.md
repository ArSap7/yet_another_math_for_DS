# Yet another math for DS course

Это страничка курса по математике для анализа данных, который читается в 2023-2024 годах в [магистратуре МОВС ФКН](https://www.hse.ru/ma/mlds/)

- Тут будет плейлист с записями на youtube

## Полезные ссылки 

- Курс по линалу от ББ
- Конкретная математика Кнута
- Ещё чего-нибдуь :)
- Продолжение курса: [yet another matstat course](https://github.com/FUlyankin/yet_another_matstat_course)

## Идеология курса

Это курс по математике для анализа данных. Мы поговорим о поднаготной ML и попытаемся не заблудиться в теоремах и доказательствах. Будет два трека: продвинутый и классный. 

- В курсе не будет АБ-тестов и матстата. Они будут в следующем семестре. 
- Везде будем пытаться втыкать примеры из практики и нормально обосновывать, нафига мы об этом говорим.
- Забыть про слово "очевидно" и везде проговаривать почему это именно так работаетб но не упарываться доказательствами.
- Больше смысла и концепций, меньше доказательств.
  
Я попытлся замиксовать в программе курса несколько удачных подходов. В логах лекций я попытался вставить ссылки на первоисточники. План ниже примерный. Он будет меняться в процессе лекций. Возможно, я пойму что какие-то куски сделаны неудачно. В программе для классной группы я ориентировался на адаптационный курс математики в ШАД. Программу для продвинутой группы я пытался собирать как справочник по математике для DS. Каждая лекция должна покрыть какую-то тему, которая всплывает где-нибудь в DS, но человек может потенциально в ней плавать. Темы из тервера подготавливают базу для матстата и АБ-тестов. О них речь пойдёт в следующем семестре. 

### План для продвинутой группы: 

1. _Матричные производные и оптимизация._ Подробно обсудим матричные производные. В машинном обучении много матриц и производных. Кажется, что это мастхэв. Попутно вспомним основные операции с матрицами.
2. _Численные методы линейной алгебры. Разреженные матрицы._ Поговорим о том, как компьютер работает с матрицами и на какие проблемы можно нарваться в контексте точности вычислений.
3. _Матричные разложения._ Обсудим SVD, QR и LU-разложения._ SVD часто всплывает в рекомендательных системах и понижении размерности, а ничего не знать про LU и QR-разложения нужно, чтобы уметь поддержать интересную беседу.
4. _Выпуклая оптимизация._ Поговорим про лагранджиан, двойственную задачу, теорему Куна-Такера и то как далеко ML находится от теориии и оптимизирует какую-то неведомую дичь вместо выпуклых няшных функций. Подоказываем какие-нибудь прикольные факты про градиентный спуск. 
5. _Рекуррентные уравнения и суммы._ Бывает, что всплывают в разных местах. Поговорим как решать и искать разные суммы.
6. _Комбинаторика, специальные числа._ Кроме биномиальных коэффициентов бывают разные другие специальные числа. Более подробно обсудим их. Возможно, порешаем комбинаторные задачки.
7. _Асимптотика._ Поговорим про сисвол O, формулу Эйлера и немного про оценку сложности алгоритмов.
8. _Тервер._ Поговорим про теорию вероятностей, будем довольно много обусуждать сигма-алгебру и на кой чёрт она нужна.
9. _Разлагай и влавствуй._ Порешаем нетривиальные задачи по терверу, научимся раскладывать случайные величины в суммы индексов и разным другим прикольным приёмам.
10. _Цепи Маркова и метод первого шага._ Порешаем задачки на цепи Маркова и обсудим, что это такое.
11. _Непрерывные случайные величины._ Вспомним что такое непрерывные случайные величины и порешаем задачи на них.
12. _Многомерные распределения и условные распределения._ 
13. _Большая сила о-малых._ Поговорим про Пуассоновский поток и откуда он берётся. 

Ближе к Новому Году будет несколько бонусных необязательных лекций: 

- Производящие функции 
- Характеристические функции
- Мартингалы и теорема Дуба
- Стохастические интегралы, Винеровский процесс
- мб чёнить про модель Блэка-Шоулза
- Тензорный поезд (маловероятно)


### План для классной группы: 

1. _Множества. Бесконечности бывают разными._ На первой лекции мы поговорим о множествах и бесконечностях. Они будут всплывать повсюду, даже в документации к SQL...
2. _Матрицы._  На второй лекции мы поговорим про матрицы. Про то, как они связаны с данными, как на них удобно смотреть и вспомним кучу полезных свойств и теорем.
3. _Определители и обратные матрицы._ Продолжаем обсуждать матрицы и операции над ними, которые часто будут всплывать а ML.  
4. _Векторные пространства, координаты, ранги, линейные отображения и операторы._ Всё современное машинное обучиние это попытка превратить объекты в какое-нибудь классное векторное пространство. Тут мы выясним что такое векторное пространство, а на машинном обучении вы выясните как превращать в него объекты.
5. _Билинейные формы, скалярное произведение и квадратичные формы._ Напрямую они в ML не всплывают, но про них часто говорят косвенно. Надо разобраться в основных определениях и терминах :) 
6. _SVD._ Сингулярное разложение постоянно всплывает в разных частях анализа данных. Например, в PCA и рекомендациях. На паре мы подробно разберёмся, как именно оно работает, какие виды у него бывают.
7. _Ликбез по пределам, рядам и производным._ Вспомним основные штуки и поподробнее поговорим про ряды. Брать производные и простые интегралы обычно все итак умеют, а штуки вроде рядов Тэйлора и производной по направлению вечно забывают.
8. _Ликбез по оптимизации._ Поговорим про Лагранджиан, матричные производные и где это всё всплывает. 
9. _Базовый тервер,Сигма-алгебра, классическая вероятность, условная вероятность, формула Байеса._ 
10. _Комбинаторика и дискретные случайные величины._ 
11. _Функция распределения. Непрерывные случайные величины._
12. _Многомерные распределения, зависимости между случайными величинами._
13. _Условные распределения и математические ожидания._

## Лицензия

Весь контент, созданный для этого курса распространяются на правах лицензии [MIT License](https://github.com/FUlyankin/yet_another_math_for_DS/blob/main/LICENSE) либо на правах лицензии [WTFPL](http://www.wtfpl.net/) на ваш выбор. Материалы публикуются как общественное достояние.
