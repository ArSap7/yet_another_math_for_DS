# Yet another math for DS course

Это страничка курса по математике для анализа данных, который читается в 2023-2024 годах в [магистратуре МОВС ФКН](https://www.hse.ru/ma/mlds/)

- Тут будет плейлист с записями на youtube

## Полезные ссылки 

- Курс по линалу от ББ
- Конкретная математика Кнута
- Ещё чего-нибдуь :)
- Продолжение курса: [yet another matstat course](https://github.com/FUlyankin/yet_another_matstat_course)

## Идеология курса

Это курс по математике для анализа данных. Мы поговорим о поднаготной ML и попытаемся не заблудиться в теоремах и доказательствах. Будет два трека: продвинутый и классный. 

- В курсе не будет АБ-тестов и матстата. Они будут в следующем семестре. 
- Везде будем пытаться искать примеры из практики и нормально обосновывать, нафига мы об этом говорим.
- Забыть про слово "очевидно" и везде проговаривать, почему это именно так работает, но не упарываться доказательствами.
- Больше смысла и концепций, меньше доказательств.

Я попытлся замиксовать в программе курса несколько подходов. В логах лекций я буду вставлять ссылки на первоисточники. План ниже примерный. Он будет меняться в процессе лекций. Возможно, я пойму что какие-то куски сделаны неудачно. 

В программе для классной группы я ориентировался на адаптационный курс математики в ШАД. Программу для продвинутой группы я пытался собирать, как справочник по математике для DS. Каждая лекция должна покрыть какую-то тему, которая всплывает где-нибудь в DS, но человек может потенциально в ней плавать. Темы из тервера подготавливают базу для матстата и АБ-тестов. О них речь пойдёт в следующем семестре. 

### План для продвинутой группы: 

1. __Матричные производные и оптимизация.__ Подробно обсудим матричные производные. В машинном обучении много матриц и производных. Кажется, что это мастхэв. Попутно вспомним основные операции с матрицами.
2. __Численные методы линейной алгебры. Разреженные матрицы.__ Поговорим о том, как компьютер работает с матрицами и на какие проблемы можно нарваться в контексте точности вычислений.
3. __Матричные разложения.__ Обсудим SVD, QR и LU-разложения._ SVD часто всплывает в рекомендательных системах и понижении размерности, а ничего не знать про LU и QR-разложения нужно, чтобы уметь поддержать интересную беседу.
4. __Выпуклая оптимизация.__ Поговорим про лагранджиан, двойственную задачу, теорему Куна-Такера и то как далеко ML находится от теориии и оптимизирует какую-то неведомую дичь вместо выпуклых няшных функций. Подоказываем какие-нибудь прикольные факты про градиентный спуск. 
5. __Рекуррентные уравнения и суммы.__ Бывает, что всплывают в разных местах. Поговорим, как решать и искать разные суммы.
6. __Комбинаторика, специальные числа.__ Кроме биномиальных коэффициентов бывают разные другие специальные числа. Более подробно обсудим их. Возможно, порешаем комбинаторные задачки.
7. __Асимптотика.__ Поговорим про сисвол O, формулу Эйлера и немного про оценку сложности алгоритмов.
8. __Тервер.__ Поговорим про теорию вероятностей, будем довольно много обусуждать сигма-алгебру и на кой чёрт она нужна.
9. __Разлагай и влавствуй.__ Порешаем нетривиальные задачи по терверу, научимся раскладывать случайные величины в суммы индексов и разным другим прикольным приёмам.
10. __Цепи Маркова и метод первого шага.__ Порешаем задачки на цепи Маркова и обсудим, что это такое.
11. __Непрерывные случайные величины.__ Вспомним что такое непрерывные случайные величины и порешаем задачи на них.
12. __Многомерные распределения и условные распределения.__ 
13. __Большая сила о-малых.__ Поговорим про Пуассоновский поток и откуда он берётся. 

Ближе к Новому Году будет несколько бонусных необязательных лекций: 

- Производящие функции 
- Характеристические функции
- VC-размерность
- Мартингалы и теорема Дуба
- Стохастические интегралы, Винеровский процесс
- мб чёнить про модель Блэка-Шоулза
- Тензорный поезд (маловероятно)

### План для классной группы: 

1. __Множества. Бесконечности бывают разными.__ На первой лекции мы поговорим о множествах и бесконечностях. Они будут всплывать повсюду, даже в документации к SQL...
2. __Матрицы.__  На второй лекции мы поговорим про матрицы. Про то, как они связаны с данными, как на них удобно смотреть и вспомним кучу полезных свойств и теорем.
3. __Определители и обратные матрицы.__ Продолжаем обсуждать матрицы и операции над ними, которые часто будут всплывать а ML.  
4. __Векторные пространства, координаты, ранги, линейные отображения и операторы.__ Всё современное машинное обучиние это попытка превратить объекты в какое-нибудь классное векторное пространство. Тут мы выясним что такое векторное пространство, а на машинном обучении вы выясните как превращать в него объекты.
5. __Билинейные формы, скалярное произведение и квадратичные формы.__ Напрямую они в ML не всплывают, но про них часто говорят косвенно. Надо разобраться в основных определениях и терминах :) 
6. __SVD.__ Сингулярное разложение постоянно всплывает в разных частях анализа данных. Например, в PCA и рекомендациях. На паре мы подробно разберёмся, как именно оно работает, какие виды у него бывают.
7. __Ликбез по пределам, рядам и производным.__ Вспомним основные штуки и поподробнее поговорим про ряды. Брать производные и простые интегралы обычно все итак умеют, а штуки вроде рядов Тэйлора и производной по направлению вечно забывают.
8. __Ликбез по оптимизации.__ Поговорим про Лагранджиан, матричные производные и где это всё всплывает. 
9. __Базовый тервер,Сигма-алгебра, классическая вероятность, условная вероятность, формула Байеса.__
10. __Комбинаторика и дискретные случайные величины.__
11. __Функция распределения. Непрерывные случайные величины.__
12. __Многомерные распределения, зависимости между случайными величинами.__
13. __Условные распределения и математические ожидания.__

## Лицензия

Весь контент, созданный для этого курса распространяются на правах лицензии [MIT License](https://github.com/FUlyankin/yet_another_math_for_DS/blob/main/LICENSE) либо на правах лицензии [WTFPL](http://www.wtfpl.net/) на ваш выбор. Материалы публикуются как общественное достояние.
